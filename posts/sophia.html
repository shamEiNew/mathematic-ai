
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SOPHIA Optimization</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .prose h3 { margin-top: 2em; margin-bottom: 0.75em; font-size: 1.5rem; font-weight: bold; color: white; }
        .prose p { margin-bottom: 1em; line-height: 1.75; color: #d1d5db; }
        .prose img { max-width: 100%; height: auto; margin: 1em 0; border-radius: 0.5rem; }
    </style>
</head>
<body class="bg-black text-white font-serif">
    <div class="min-h-screen flex flex-col">
         <header class="border-b border-gray-700">
            <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-6">
                <a href="../index.html" class="text-4xl font-bold font-sans hover:text-gray-300 transition-colors duration-300">
                    Mathematics & AI
                </a>
            </div>
        </header>
        <main class="flex-grow container mx-auto px-4 sm:px-6 lg:px-8 py-8">
            <article class="max-w-4xl mx-auto">
                <a href="../index.html" class="text-gray-400 font-bold inline-block mb-8 hover:text-white">&larr; Back</a>
                <h1 class="text-4xl md:text-5xl font-bold font-sans leading-tight mb-4">Second-order Clipped Stochastic Optimization (SOPHIA)</h1>
                <div class="text-md text-gray-400 border-b border-gray-700 pb-4 mb-8">
                    <span>June 9, 2023</span> &bull; <span>By Sham</span>
                    <span class="ml-2 text-xs border border-gray-700 rounded px-2 py-0.5">Explore</span>
                </div>
                <div class="prose prose-invert prose-lg">
                    <p>Sophia, a second-order clipped optimizer was introduced recently by a group at Stanford. The paper specifically claims that training large language models (LLMs) can be made less expensive using this optimizer with fewer number of steps compared to most widely used Adam or its variants.</p>

                    <p>For example the toy functions:</p>

                    <p>$$ L_1(\theta_{[1]})= 8(\theta_{[1]}-1)^2 (1.3\theta_{[1]}+2\theta_{[1]} + 2\theta{[1]}+1) $$</p>
                    <p>$$ L_2(\theta_{[2]}) = \frac{1}{2(\theta_{[2]}-4)^2} $$</p>

                    <p>The method uses Hessian information to guide the updates, potentially navigating the loss landscape more efficiently than first-order methods.</p>
                </div>
            </article>
        </main>
    </div>
</body>
</html>
